<h1 align="center">
  <br />
  AI for Cybersecurity Attribution
  <br />

### ðŸ¤– Fine-Tuning for Small Language Models on Synthetic Data to Enhance APT Attribution & TTP Chaining

<p align="justify"> A fundamental challenge in modern cybersecurity is the accurate attribution of Advanced Persistent Threats (APTs), which are sophisticated entities that conduct long-term, stealthy intrusions. Traditional forensic methods that rely on tracking static Indicators of Compromise (IoCs)â€”such as IP addresses or file hashesâ€”are inherently fragile, as these artifacts can be trivially altered by a capable adversary. Similarly, malware lineage analysis, which traces the evolution of malicious code, can be defeated by code obfuscation or the use of novel implants. A more resilient and modern paradigm, therefore, shifts the focus from an attacker's transient tools to their observable behaviors. This is enabled by the MITRE ATT&CK framework, which provides a globally recognized ontology of adversary actions. This framework catalogs behaviors into a hierarchy of Tactics (the adversary's tactical goals, like 'Privilege Escalation'), Techniques (the methods to achieve those goals, like 'Exploiting a vulnerable service'), and Procedures (the specific implementation of a technique by an actor), all of which can be mapped along a logical kill chain sequence.</p>

<p align="justify">The primary obstacle to building behavioral attribution models is the profound scarcity of well-labeled, publicly available attack data. A powerful solution to this data bottleneck is the use of generative AI to create a high-fidelity synthetic corpus. In such a system, a Large Language Model (LLM) like GPTâˆ’4, with its vast knowledge and reasoning capabilities, can be prompted to generate realistic, analyst-style narratives that embed specific TTPs in plausible sequences. However, raw generated data requires careful refinement to be trustworthy. The TFâˆ’IDF (Term Frequency-Inverse Document Frequency) algorithm can be applied to analyze the generated narratives, calculating a distinctiveness score for each technique to automatically identify and emphasize rare behaviors that are highly indicative of a specific actor. Furthermore, to guard against AI "hallucination" or illogical event chains, a NaÃ¯ve Bayes classifier can serve as an efficient probabilistic filter. Despite its simplifying assumption that all features are independent, it effectively assesses the likelihood of a given TTP sequence, pruning unrealistic narratives and enhancing the overall quality and logical coherence of the training data.</p>

<p align="justify">
With a robust synthetic dataset, the final step is to build an efficient and deployable attribution engine. While a large LLM is ideal for data generation, a Small Language Model (SLM), such as LlamaÂ 3.1Â 8B, offers a much better balance of performance and resource consumption for the final deployed tool. This smaller model is then specialized for the attribution task through fine-tuning. To make this process accessible and cost-effective, advanced efficiency techniques are critical. LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that freezes the vast majority of the model's original parameters and only trains small, injectable "adapter" layers, drastically reducing memory and compute requirements. This efficiency is further amplified by 4âˆ’bit quantization, a compression technique that reduces the model's memory footprint, enabling it to run on standard, commodity GPUs. A system built with this architecture is not only accessible to organizations without massive compute budgets but is also inherently agile. This allows for rapid retraining to counter concept driftâ€”the inevitable evolution of attacker methodsâ€”far surpassing the static nature of older sequence-matching and malware-clustering systems.</p>

#### ðŸ“¢ Coming Soon
<p align="justify"> ðŸ“„ A detailed research publication with methodology, experiments, and benchmarks will be released soon.</p>

[Back to the Project List](https://github.com/ntust-im-labyrinth/labyrinth/tree/GilvyThelmaProjectM/projects#----projects---colorbluelab-coloryellowy-oung--colororanger-estless-colorgreenin-colorredt-hreat-colororangeh-unting)
